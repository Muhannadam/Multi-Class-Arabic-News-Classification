import streamlit as st
import joblib
import re
import os
import requests
import nltk
from nltk.corpus import stopwords

# Groq API Key (from environment variable)
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
model = joblib.load('baseline_lr_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚ÙÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
try:
    stopwords.words('arabic')
except LookupError:
    nltk.download('stopwords')

arabic_stopwords = set(stopwords.words('arabic'))

# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def clean_text(text):
    def remove_tashkeel(t): return re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', t)
    def remove_repeated_chars(t): return re.sub(r'(.)\1{2,}', r'\1\1', t)

    text = remove_tashkeel(text)
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)
    text = re.sub(r'[\d\u0660-\u0669]+', ' ', text)
    text = remove_repeated_chars(text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = [w for w in text.split() if w not in arabic_stopwords and len(w) > 1]
    return ' '.join(tokens)

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Groq API
def summarize_and_suggest_title(text):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "allam-2-7b",
        "messages": [
            {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ. Ø¹Ù†Ø¯Ù…Ø§ ÙŠØµÙ„Ùƒ Ù†Øµ Ø·ÙˆÙŠÙ„ØŒ Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµÙ‡ Ø¨Ø´ÙƒÙ„ Ù…Ø®ØªØµØ±ØŒ Ø«Ù… Ø§Ù‚ØªØ±Ø­ Ø¹Ù†ÙˆØ§Ù†Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ ÙˆØ¬Ø°Ø§Ø¨Ù‹Ø§ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."},
            {"role": "user", "content": f"Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„:\n\n{text}\n\nØ±Ø¬Ø§Ø¡Ù‹: 1- Ù„Ø®Øµ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙÙŠ ÙÙ‚Ø±Ø© Ù‚ØµÙŠØ±Ø©. 2- Ø§Ù‚ØªØ±Ø­ Ø¹Ù†ÙˆØ§Ù†Ù‹Ø§ Ø°ÙƒÙŠÙ‹Ø§ Ù„Ù„Ù…Ù‚Ø§Ù„."}
        ],
        "temperature": 0.5,
        "max_tokens": 500
    }
    try:
        response = requests.post(url, headers=headers, json=payload)
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"].strip()
        else:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {response.status_code} - {response.text}"
    except Exception as e:
        return f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {str(e)}"

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
st.title("ğŸ” Arabic News Classifier (Logistic Regression + Groq AI)")

input_text = st.text_area("âœï¸ Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£Ùˆ Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ù‡Ù†Ø§", height=200)

if st.button("ğŸ” ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚Ø§Ù„"):
    if input_text.strip() == "":
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ.")
    else:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØªØµÙ†ÙŠÙÙ‡
        cleaned = clean_text(input_text)
        tfidf_input = vectorizer.transform([cleaned])
        pred = model.predict(tfidf_input)
        label = label_encoder.inverse_transform(pred)[0]
        st.success(f"âœ… Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: **{label}**")

        # Ø§Ù„ØªÙ„Ø®ÙŠØµ ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        with st.spinner("âœï¸ Ø¬Ø§Ø±ÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø®Ø¨Ø± ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø¹Ù†ÙˆØ§Ù†..."):
            summary_output = summarize_and_suggest_title(input_text)
            st.subheader("ğŸ“ ØªÙ„Ø®ÙŠØµ ÙˆØ¹Ù†ÙˆØ§Ù† Ù…Ù‚ØªØ±Ø­:")
            st.markdown(summary_output)
